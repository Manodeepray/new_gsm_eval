{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5EqFUa-CLfFN",
        "Fje__OlvLjUd",
        "85_utrl7LlfC",
        "xlOs-UsKLn3k",
        "h4OEARpkLqjC",
        "Of8qyzvYKvII",
        "n91Drhi6K31X",
        "4vnPBD9kgbdJ",
        "m3SUQPgqg8lY",
        "s1shYrzdgh0c",
        "D9aEbsoOg-kv",
        "nIi9lKHJgk80",
        "EzJPc_nRhBRf",
        "tkox3xIGhDXS",
        "JPSnX-d6gqHW",
        "ivwb1PCShPY5",
        "iINWgz36hRxq",
        "SafkSAdqgwR-",
        "ijaZcdAshiJJ",
        "DF6wNrkthj9e",
        "UHCJKPU8g1gc",
        "XTMLDWWhhzRF",
        "rdC5jLDzg4p7",
        "_svhi0Q7h1qD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## install dependencies"
      ],
      "metadata": {
        "id": "5EqFUa-CLfFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa4FBMKOLCjF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install unsloth-zoo\n",
        "!pip install kagglehub\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /gsm-dataset-orignal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB7FwFo2fmfR",
        "outputId": "4dd7de98-a770-4611-b2a8-d1663b311e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/gsm-dataset-orignal'\n",
            "/root/.cache/kagglehub/datasets/raymanodeep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load imports"
      ],
      "metadata": {
        "id": "Fje__OlvLjUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import TextStreamer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYewK5wHVFAP",
        "outputId": "15ce6882-7d9a-4227-fcaf-d1d16cc671c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download dataset"
      ],
      "metadata": {
        "id": "85_utrl7LlfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replaced_dataset_path = kagglehub.dataset_download('raymanodeep/finetune-1-prime-nos')\n",
        "original_dataset_path = kagglehub.dataset_download('raymanodeep/gsm-dataset-orignal')\n",
        "\n",
        "original_dataset_path,replaced_dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZfawne8ce9o",
        "outputId": "555ae7ad-a592-45fd-f0a2-07da5d59625a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/raymanodeep/finetune-1-prime-nos?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50M/3.50M [00:00<00:00, 185MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/raymanodeep/gsm-dataset-orignal?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.30M/3.30M [00:00<00:00, 167MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/root/.cache/kagglehub/datasets/raymanodeep/gsm-dataset-orignal/versions/1',\n",
              " '/root/.cache/kagglehub/datasets/raymanodeep/finetune-1-prime-nos/versions/1')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mount drive"
      ],
      "metadata": {
        "id": "xlOs-UsKLn3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive\"\n",
        "os.mkdir(path = os.path.join(drive_path ,\"inference_evaluation\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-vc7bdcVS-c",
        "outputId": "b297d6b3-4552-4e59-edaf-1651941a0f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set torch device"
      ],
      "metadata": {
        "id": "h4OEARpkLqjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "torch.device(device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8HLYdkmVZBk",
        "outputId": "4e106912-09a3-49f0-cebb-d2d5b33cfb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load replaced dataset"
      ],
      "metadata": {
        "id": "Of8qyzvYKvII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "|import json\n",
        "replaced_data = []\n",
        "replaced_test_data = []\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/replaced_dataset/test_output.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        replaced_test_data.append(json.loads(line.strip()))\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/replaced_dataset/train_output.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        replaced_data.append(json.loads(line.strip()))\n",
        "\n",
        "\n",
        "\n",
        "replaced_socratic_data = []\n",
        "replaced_socratic_test_data =[]\n",
        "with open(str(replaced_dataset_path) +\"/replaced_dataset/test_socratic_output.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        replaced_socratic_test_data.append(json.loads(line.strip()))\n",
        "\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/replaced_dataset/train_socratic_output.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        replaced_socratic_data.append(json.loads(line.strip()))"
      ],
      "metadata": {
        "id": "ALq30V-4W81O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load original dataset"
      ],
      "metadata": {
        "id": "n91Drhi6K31X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "orig_data = []\n",
        "orig_test_data = []\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/test.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        orig_test_data.append(json.loads(line.strip()))\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/train.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        orig_data.append(json.loads(line.strip()))\n",
        "\n",
        "\n",
        "\n",
        "orig_socratic_data = []\n",
        "orig_socratic_test_data =[]\n",
        "with open(str(replaced_dataset_path) +\"/test_socratic.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        orig_socratic_test_data.append(json.loads(line.strip()))\n",
        "\n",
        "\n",
        "with open(str(replaced_dataset_path) +\"/train_socratic.jsonl\",\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Parse each line as a JSON object and append to the list\n",
        "        orig_socratic_data.append(json.loads(line.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NEpGmvd5bVar",
        "outputId": "9a1a4a37-59c5-4cd2-b3b5-439ba86dc4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'replaced_dataset_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb4dfa2ddf47>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0morig_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/test.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Parse each line as a JSON object and append to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'replaced_dataset_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llama 3"
      ],
      "metadata": {
        "id": "4vnPBD9kgbdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8B"
      ],
      "metadata": {
        "id": "m3SUQPgqg8lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "|from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "pYh89nWweyMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "llama3_8b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_8b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "VNDhNR-KiGn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "llama3_8b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_8b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_8b_test_results = {}\n",
        "llama3_8b_results[\"test_results\"]=llama3_8b_test_answers\n",
        "llama3_8b_results[\"replaced_test_answers\"] = llama3_8b_replaced_test_answers\n",
        "\n",
        "llama3_8b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_8b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_8b_dir,\"llama3_8b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_8b_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "37HWUTMXiWgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "llama3_8b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_8b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "S1KRilq5vd8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "llama3_8b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_8b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_8b_test_socratic_results = {}\n",
        "llama3_8b_test_socratic_results[\"test_results\"]=llama3_8b_test_socratic_answers\n",
        "llama3_8b_test_socratic_results[\"replaced_test_results\"] = llama3_8b_replaced_test_socratic_answers\n",
        "\n",
        "llama3_8b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_8b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_8b_dir,\"llama3_8b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_8b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "P0Wnwdc2wqU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31IbVczH6CIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llama 3.1"
      ],
      "metadata": {
        "id": "s1shYrzdgh0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8B"
      ],
      "metadata": {
        "id": "D9aEbsoOg-kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "lk5HOmHxgkdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "llama3_1_8b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_1_8b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "EWE9GWUy5jcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "llama3_1_8b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_1_8b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_1_8b_test_results = {}\n",
        "llama3_1_8b_test_results[\"test_results\"]=llama3_1_8b_test_answers\n",
        "llama3_1_8b_test_results[\"replaced_test_answers\"] = llama3_1_8b_replaced_test_answers\n",
        "\n",
        "llama3_1_8b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_1_8b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_1_8b_dir,\"llama3_1_8b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_1_8b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "7Kd6Em9t5jcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "llama3_1_8b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_1_8b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "PQzOQR3N5jcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "llama3_1_8b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_1_8b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_1_8b_test_socratic_results = {}\n",
        "llama3_1_8b_test_socratic_results[\"test_results\"]=llama3_1_8b_test_socratic_answers\n",
        "llama3_1_8b_test_socratic_results[\"finetuned_results\"] = llama3_1_8b_replaced_test_socratic_answers\n",
        "\n",
        "llama3_1_8b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_1_8b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_1_8b_dir,\"llama3_1_8b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_1_8b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "62vXmp6w5jcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llama 3.2"
      ],
      "metadata": {
        "id": "nIi9lKHJgk80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1B"
      ],
      "metadata": {
        "id": "EzJPc_nRhBRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "LfRIAS2sgpe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "llama3_2_1b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_1b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "3e616pf6EyNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "llama3_2_1b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_1b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_2_1b_test_results = {}\n",
        "llama3_2_1b_test_results[\"test_results\"]=llama3_2_1b_test_answers\n",
        "llama3_2_1b_test_results[\"replaced_test_answers\"] = llama3_2_1b_replaced_test_answers\n",
        "\n",
        "llama3_2_1b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_2_1b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_2_1b_dir,\"llama3_2_1b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_2_1b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "4JA9_34SEyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "llama3_2_1b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_1b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "m6CW_x2fEyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "llama3_2_1b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_1b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_2_1b_test_socratic_results = {}\n",
        "llama3_2_1b_test_socratic_results[\"test_results\"]=llama3_2_1b_test_socratic_answers\n",
        "llama3_2_1b_test_socratic_results[\"replaced_test_results\"] = llama3_2_1b_replaced_test_socratic_answers\n",
        "\n",
        "llama3_2_1b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_2_1b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_2_1b_dir,\"llama3_2_1b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_2_1b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "CNdOnvyvEyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3B"
      ],
      "metadata": {
        "id": "tkox3xIGhDXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "8jL4ySPrhF6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "llama3_2_3b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_3b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "g62jTKDDE082"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "llama3_2_3b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_3b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_2_3b_test_results = {}\n",
        "llama3_2_3b_test_results[\"test_results\"]=llama3_2_3b_test_answers\n",
        "llama3_2_3b_test_results[\"replaced_test_answers\"] = llama3_2_3b_replaced_test_answers\n",
        "\n",
        "llama3_2_3b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_2_3b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_2_3b_dir,\"llama3_2_3b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_2_3b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "34c-joZIE088"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "llama3_2_3b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_3b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "EOHnCeXsE088"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "llama3_2_3b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  llama3_2_3b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "llama3_2_1b_test_socratic_results = {}\n",
        "llama3_2_1b_test_socratic_results[\"test_results\"]=llama3_2_3b_test_socratic_answers\n",
        "llama3_2_1b_test_socratic_results[\"replaced_test_results\"] = llama3_2_3b_replaced_test_socratic_answers\n",
        "\n",
        "llama3_2_3b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"llama3_2_3b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(llama3_2_3b_dir,\"llama3_2_1b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(llama3_2_1b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "8GtoGgUOE089"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minstral"
      ],
      "metadata": {
        "id": "JPSnX-d6gqHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7B"
      ],
      "metadata": {
        "id": "ivwb1PCShPY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-v0.3\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "0kso_AYpgseW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "minstral_7b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_7b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "wVBgzNfOE1L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "minstral_7b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_7b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "minstral_7b_test_results = {}\n",
        "minstral_7b_test_results[\"test_results\"]=minstral_7b_orig_test_answers\n",
        "minstral_7b_test_results[\"replaced_test_answers\"] = minstral_7b_replaced_test_answers\n",
        "\n",
        "minstral_7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"minstral_7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(minstral_7b_dir,\"minstral_7b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(minstral_7b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "mNmZZKUeE1L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "minstral_7b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_7b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "k9SnthkQE1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "minstral_7b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_7b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "minstral_7b_test_socratic_results = {}\n",
        "minstral_7b_test_socratic_results[\"test_results\"] = minstral_7b_test_socratic_answers\n",
        "minstral_7b_test_socratic_results[\"replaced_test_results\"] = minstral_7b_replaced_test_socratic_answers\n",
        "\n",
        "minstral_7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"minstral_7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(minstral_7b_dir,\"minstral_7b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(minstral_7b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "gKP71_R3E1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12B (nemo 2407)"
      ],
      "metadata": {
        "id": "iINWgz36hRxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "XTLKmNYFhUlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "minstral_12b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_12b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "4moJDL9GF89f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "minstral_12b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_12b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "minstral_12b_test_results = {}\n",
        "minstral_12b_test_results[\"test_results\"]=minstral_12b_orig_test_answers\n",
        "minstral_12b_test_results[\"replaced_test_answers\"] = minstral_12b_replaced_test_answers\n",
        "\n",
        "minstral_12b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"minstral_12b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(minstral_12b_dir,\"minstral_12b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(minstral_12b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "1RvcRmnWF89f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "minstral_12b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_12b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "SKaTC3vvF89f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "minstral_12b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  minstral_12b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "minstral_12b_test_socratic_results = {}\n",
        "minstral_12b_test_socratic_results[\"test_results\"] = minstral_12b_test_socratic_answers\n",
        "minstral_12b_test_socratic_results[\"replaced_test_results\"] = minstral_12b_replaced_test_socratic_answers\n",
        "\n",
        "minstral_12b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"minstral_12b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(minstral_12b_dir,\"minstral_12b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(minstral_12b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "D6I5nO2RF89f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## qwen2"
      ],
      "metadata": {
        "id": "SafkSAdqgwR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5B"
      ],
      "metadata": {
        "id": "ijaZcdAshiJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name =  \"unsloth/Qwen2-1.5b-bnb-4bit\", # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "l5wsSDKlgxyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "qwen_2_1d5b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_1d5b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "18LUXz-hI0JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "qwen_2_1d5b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_1d5b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "qwen_2_1d5b_test_results = {}\n",
        "qwen_2_1d5b_test_results[\"test_results\"]=qwen_2_1d5b_orig_test_answers\n",
        "qwen_2_1d5b_test_results[\"replaced_test_answers\"] = qwen_2_1d5b_replaced_test_answers\n",
        "\n",
        "qwen_2_1d5b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"qwen_2_1d5b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(minstral_7b_dir,\"qwen_2_1d5b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(qwen_2_1d5b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "Kn5PMnFKI0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "qwen_2_1d5b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_1d5b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "AgXkfUSUI0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "qwen_2_1d5b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_1d5b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "qwen_2_1d5b_test_socratic_results = {}\n",
        "qwen_2_1d5b_test_socratic_results[\"test_results\"] = qwen_2_1d5b_test_socratic_answers\n",
        "qwen_2_1d5b_test_socratic_results[\"replaced_test_results\"] = qwen_2_1d5b_replaced_test_socratic_answers\n",
        "\n",
        "qwen_2_1d5b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"qwen_2_1d5b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(qwen_2_1d5b_dir,\"qwen_2_1d5b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(qwen_2_1d5b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "AdZIWBmUI0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7B"
      ],
      "metadata": {
        "id": "DF6wNrkthj9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2-7B\", # Reminder we support ANY Hugging Face model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "0SuSXIY3hlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "qwen_2_7b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_7b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "zrftdEEsJmf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "qwen_2_7b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_7b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "qwen_2_7b_test_results = {}\n",
        "qwen_2_7b_test_results[\"test_results\"]=qwen_2_7b_orig_test_answers\n",
        "qwen_2_7b_test_results[\"replaced_test_answers\"] = qwen_2_7b_replaced_test_answers\n",
        "\n",
        "qwen_2_7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"qwen_2_7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(qwen_2_7b_dir,\"qwen_2_7b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(qwen_2_7b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "6xPqNgwWJmgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "qwen_2_7b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_7b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "coQL8km-JmgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "qwen_2_7b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  qwen_2_7b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "qwen_2_7b_test_socratic_results = {}\n",
        "qwen_2_7b_test_socratic_results[\"test_results\"] = qwen_2_7b_test_socratic_answers\n",
        "qwen_2_7b_test_socratic_results[\"replaced_test_results\"] = qwen_2_7b_replaced_test_socratic_answers\n",
        "\n",
        "qwen_2_7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"qwen_2_7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(qwen_2_7b_dir,\"qwen_2_7b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(qwen_2_7b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "HiAfG5TCJmgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEMMA 2"
      ],
      "metadata": {
        "id": "UHCJKPU8g1gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2B"
      ],
      "metadata": {
        "id": "T25cL_Y0hxkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-2b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ],
      "metadata": {
        "id": "J6zVnffOg310"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "gemma_2_2b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_2b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "tPs43b8hM5uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "gemma_2_2b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_2b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "gemma_2_2b_test_results = {}\n",
        "gemma_2_2b_test_results[\"test_results\"]=gemma_2_2b_orig_test_answers\n",
        "gemma_2_2b_test_results[\"replaced_test_answers\"] = gemma_2_2b_replaced_test_answers\n",
        "\n",
        "gemma_2_2b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"gemma_2_2b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(gemma_2_2b_dir,\"gemma_2_2b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(gemma_2_2b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "6i9WtQSBM5uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "gemma_2_2b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_2b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "jOZsZpJqM5uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "gemma_2_2b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_2b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "gemma_2_2b_test_socratic_results = {}\n",
        "gemma_2_2b_test_socratic_results[\"test_results\"] = gemma_2_2b_test_socratic_answers\n",
        "gemma_2_2b_test_socratic_results[\"replaced_test_results\"] = gemma_2_2b_replaced_test_socratic_answers\n",
        "\n",
        "gemma_2_2b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"gemma_2_2b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(gemma_2_2b_dir,\"gemma_2_2b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(gemma_2_2b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "EUzHwitPM5uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9B"
      ],
      "metadata": {
        "id": "XTMLDWWhhzRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "lQzRyXQtMRUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "gemma_2_9b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_9b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "FpzggwSCNZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "gemma_2_9b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_9b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "gemma_2_9b_test_results = {}\n",
        "gemma_2_9b_test_results[\"test_results\"]=gemma_2_9b_orig_test_answers\n",
        "gemma_2_9b_test_results[\"replaced_test_answers\"] = gemma_2_9b_replaced_test_answers\n",
        "\n",
        "gemma_2_9b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"gemma_2_9b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(gemma_2_9b_dir,\"gemma_2_9b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(gemma_2_9b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "r_la0mG5NZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "gemma_2_9b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_9b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "ffujRqRxNZiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "gemma_2_9b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  gemma_2_9b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "gemma_2_9b_test_socratic_results = {}\n",
        "gemma_2_9b_test_socratic_results[\"test_results\"] = gemma_2_9b_test_socratic_answers\n",
        "gemma_2_9b_test_socratic_results[\"replaced_test_results\"] = gemma_2_9b_replaced_test_socratic_answers\n",
        "\n",
        "gemma_2_9b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"gemma_2_9b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(gemma_2_9b_dir,\"gemma_2_9b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(gemma_2_9b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "nV0tBiSjNZiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmolLM 2"
      ],
      "metadata": {
        "id": "rdC5jLDzg4p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7B"
      ],
      "metadata": {
        "id": "_svhi0Q7h1qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/SmolLM2-1.7B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "S8CRvo6_g58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test original\n",
        "smollm_1n7b_orig_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(orig_test_data))):\n",
        "\n",
        "  item = orig_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  smollm_1n7b_orig_test_answers.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "MRJ9bdTfM6lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test replaced\n",
        "smollm_1n7b_replaced_test_answers = []\n",
        "\n",
        "for i in tqdm(range(len(replaced_test_data))):\n",
        "\n",
        "  item = replaced_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  smollm_1n7b_replaced_test_answers.append(response)\n",
        "\n",
        "\n",
        "smollm_1n7b_test_results = {}\n",
        "smollm_1n7b_test_results[\"test_results\"]=smollm_1n7b_orig_test_answers\n",
        "smollm_1n7b_test_results[\"replaced_test_answers\"] = smollm_1n7b_replaced_test_answers\n",
        "\n",
        "smollm_1n7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"smollm_1n7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(smollm_1n7b_dir,\"smollm_1n7b_test_results.json\"), \"w\") as outfile:\n",
        "    json.dump(smollm_1n7b_test_results, outfile)\n",
        "\n"
      ],
      "metadata": {
        "id": "8oSR1P3zM6lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test socratic original\n",
        "\n",
        "\n",
        "smollm_1n7b_test_socratic_answers = []\n",
        "for i in tqdm(range(len(orig_socratic_test_data))):\n",
        "\n",
        "  item = orig_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  smollm_1n7b_test_socratic_answers.append(response)\n"
      ],
      "metadata": {
        "id": "4H7vFwXcM6lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test socratic replaced original\n",
        "\n",
        "smollm_1n7b_replaced_test_socratic_answers = []\n",
        "for i in tqdm(range(len(replaced_socratic_test_data))):\n",
        "\n",
        "  item = replaced_socratic_test_data[i]\n",
        "  prompt = \"solve the following grade school math problem question : \"+item['question'] + EOS_TOKEN\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "  outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  smollm_1n7b_replaced_test_socratic_answers.append(response)\n",
        "\n",
        "\n",
        "smollm_1n7b_test_socratic_results = {}\n",
        "smollm_1n7b_test_socratic_results[\"test_results\"] = smollm_1n7b_test_socratic_answers\n",
        "smollm_1n7b_test_socratic_results[\"replaced_test_results\"] = smollm_1n7b_replaced_test_socratic_answers\n",
        "\n",
        "smollm_1n7b_dir = os.path.join(drive_path ,\"inference_evaluation\" , \"smollm_1n7b\")\n",
        "\n",
        "\n",
        "with open( os.path.join(smollm_1n7b_dir,\"smollm_1n7b_test_socratic_results.json\"), \"w\") as outfile:\n",
        "    json.dump(smollm_1n7b_test_socratic_results, outfile)\n"
      ],
      "metadata": {
        "id": "RfnSy-WnM6lc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}